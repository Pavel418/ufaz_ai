{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import mpl_toolkits\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['link'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39m#train.drop(['link'], axis=1)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m columns \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m----> 7\u001b[0m train\u001b[39m.\u001b[39mjoin(test[columns])\n\u001b[0;32m      9\u001b[0m fields \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mpublication_date\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlatitude\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlongitude\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfloor\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39marea\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mkupcha\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrooms\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrepairs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhypothec\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlocation attributes\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     10\u001b[0m \u001b[39m#if price:\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Pavel\\Documents\\GitHub\\ufaz_ai\\.conda\\lib\\site-packages\\pandas\\core\\frame.py:3811\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3809\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3810\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 3811\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   3813\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Pavel\\Documents\\GitHub\\ufaz_ai\\.conda\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6113\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   6111\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6113\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6115\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   6116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6117\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Pavel\\Documents\\GitHub\\ufaz_ai\\.conda\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6176\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6175\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 6176\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['link'] not in index\""
     ]
    }
   ],
   "source": [
    "#def process(file_name, price):\n",
    "test = pd.read_csv('test_binaaz_updated.csv')\n",
    "test['price'] = 0\n",
    "train = pd.read_csv('modified_train_binaaz.csv')\n",
    "train.drop(['link'], axis=1)\n",
    "columns = train.columns.tolist()\n",
    "train.join(test[columns])\n",
    "\n",
    "fields = ['publication_date', 'latitude', 'longitude', 'category', 'floor', 'area', 'kupcha', 'rooms', 'repairs', 'hypothec', 'location attributes']\n",
    "#if price:\n",
    "fields.append('price')\n",
    "train = train[fields]\n",
    "train.replace('var', True, inplace=True)\n",
    "train.replace('yoxdur', False, inplace=True)\n",
    "train.replace(np.nan, False, inplace=True)\n",
    "train.replace({'m²': ''}, regex=True, inplace=True)\n",
    "train.replace('Yeni tikili', True, inplace=True)\n",
    "train.replace('Köhnə tikili', False, inplace=True)\n",
    "train = train.astype({\"area\": float})\n",
    "currentFloors = []\n",
    "maxFloors = []\n",
    "for floor in train['floor'].array:\n",
    "    floors = floor.split(' / ')\n",
    "    currentFloors.append(floors[0])\n",
    "    maxFloors.append(floors[1])\n",
    "train[\"current_floor\"] = currentFloors\n",
    "train[\"max_floor\"] = maxFloors\n",
    "\n",
    "train = train.astype({\"current_floor\": int})\n",
    "train = train.astype({\"max_floor\": int})\n",
    "del train['floor']\n",
    "\n",
    "def get_normalized_series(df, col):\n",
    "    return (df[col] - df[col].mean()) / df[col].std()\n",
    "    #train[\"NormArea\"]= get_normalized_series(train, \"area\")\n",
    "    #train[\"NormCurrent\"]= get_normalized_series(train, \"current_floor\")\n",
    "    #train[\"NormMax\"]= get_normalized_series(train, \"max_floor\")\n",
    "    #if price:\n",
    "       #train[\"NormPrice\"]= get_normalized_series(train, \"price\")\n",
    "       #del train['price']\n",
    "    #del train['area']\n",
    "    #del train['current_floor']\n",
    "    #del train['max_floor']\n",
    "for index in range(0, len(train['publication_date'].array)):\n",
    "    element = train['publication_date'].array[index]\n",
    "    values = element.split(' ')\n",
    "    if values[1] == 'Yanvar':\n",
    "        values[1] = '/01/'\n",
    "    elif values[1] == 'Dekabr':\n",
    "        values[1] = '/12/'\n",
    "    train['publication_date'].array[index] = values[0] + values[1] + values[2]\n",
    "\n",
    "train['publication_date'] = pd.to_datetime(train.publication_date, format=\"%d/%m/%Y\", errors='coerce')\n",
    "train[\"year\"] = train[\"publication_date\"].dt.year\n",
    "train[\"month\"] = train[\"publication_date\"].dt.month\n",
    "train[\"day\"] = train[\"publication_date\"].dt.day\n",
    "    #return train\n",
    "#train = process('modified_train_binaaz.csv', True)\n",
    "tags = train[\"location attributes\"]\n",
    "clean_tags = tags.str.strip('[]\\'').str.split('\\', \\'')\n",
    "#tag_columns = pd.get_dummies(clean_tags.apply(pd.Series).stack()).groupby(level=0).sum()\n",
    "a = clean_tags.apply(pd.Series)\n",
    "b = a.stack()\n",
    "c = pd.get_dummies(b)\n",
    "d = c.groupby(level=0).sum()\n",
    "\n",
    "train = train.join(d)\n",
    "train = train.drop(['location attributes', 'publication_date'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"together.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = process('test_binaaz_updated.csv', False)\n",
    "tags = test[\"location attributes\"]\n",
    "test_clean_tags = tags.str.strip('[]\\'').str.split('\\', \\'')\n",
    "#tag_columns = pd.get_dummies(test_clean_tags.apply(pd.Series).stack()).groupby(level=0).sum()\n",
    "a = test_clean_tags.apply(pd.Series)\n",
    "b = a.stack()\n",
    "c = pd.get_dummies(b)\n",
    "d = c.groupby(level=0).sum()\n",
    "\n",
    "tags = set()\n",
    "for ls in clean_tags:\n",
    "   for tag in ls:\n",
    "    tags.add(tag)\n",
    "\n",
    "test_tags = set()\n",
    "for ls in test_clean_tags:\n",
    "   for tag in ls:\n",
    "    test_tags.add(tag)\n",
    "\n",
    "to_add = tags.difference(test_tags)\n",
    "empty = []\n",
    "for i in range(0, len(d.index)):\n",
    "    empty.append(0)\n",
    "\n",
    "map = {'price': empty}\n",
    "for add in to_add:\n",
    "    map[add] = empty\n",
    "\n",
    "additional = pd.DataFrame(map)\n",
    "\n",
    "test = test.join(d)\n",
    "test = test.join(additional)\n",
    "to_drop = ['location attributes', 'publication_date'] + list(test_tags.difference(tags))\n",
    "test = test.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"test_clear.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "target = \"price\"\n",
    "columns = train.columns.tolist()\n",
    "columns = [c for c in columns if c not in [\"price\"]]\n",
    "#training = train.sample(frac=0.7, random_state=5)\n",
    "#lin_model = LinearRegression()\n",
    "# Fit the model to the training data.\n",
    "#lin_model.fit(train[columns], train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our predictions for the test set.\n",
    "#test = train.loc[~train.index.isin(training.index)]\n",
    "#lin_predictions = lin_model.predict(test[columns])\n",
    "# Compute error between our test predictions and the actual values.\n",
    "#lin_mse = mean_squared_error(lin_predictions, test[target], squared=False)\n",
    "#print(\"Computed error:\", lin_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dzinski/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:292: UserWarning: The total space of parameters 6 is smaller than n_iter=12. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.838726</td>\n",
       "      <td>0.041361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.85</td>\n",
       "      <td>-0.836915</td>\n",
       "      <td>0.046917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.836839</td>\n",
       "      <td>0.046674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.834879</td>\n",
       "      <td>0.046136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.9</td>\n",
       "      <td>-0.833659</td>\n",
       "      <td>0.047175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.824562</td>\n",
       "      <td>0.046748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_subsample  mean_test_error  std_test_error\n",
       "3             0.8        -0.838726        0.041361\n",
       "4            0.85        -0.836915        0.046917\n",
       "2            0.75        -0.836839        0.046674\n",
       "1             0.7        -0.834879        0.046136\n",
       "5             0.9        -0.833659        0.047175\n",
       "0             0.6        -0.824562        0.046748"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import loguniform\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = {\n",
    "    #\"n_estimators\": [5, 10, 20, 50, 100, 200, 300, 400, 500, 600,650, 700, 800, 900, 1000],\n",
    "    #\"max_leaf_nodes\": [2, 3, 4, 5, 10, 20, 50, 100, 150, 200, 250],\n",
    "    #\"learning_rate\": loguniform(0.01, 1),\n",
    "    #\"min_samples_split\": [10, 25, 50, 75, 100, 125, 150, 175, 200, 225, 250]\n",
    "    #\"max_depth\":range(9,20,2),\n",
    "    #\"min_samples_split\": list(range(5, 25, 5)) + list(range(25, 100, 10)) + [125, 150, 200, 250, 300]\n",
    "    #'max_features': range(47, 52, 3)\n",
    "    #\"subsample\": [0.6,0.7,0.75,0.8,0.85,0.9]\n",
    "}\n",
    "search_cv = RandomizedSearchCV(\n",
    "    GradientBoostingRegressor(max_features=50, n_estimators = 900, min_samples_split=25, max_depth=15, max_leaf_nodes= 50), param_distributions=param_distributions,\n",
    "     n_iter=12, random_state=0, n_jobs=2,\n",
    ")\n",
    "search_cv.fit(train[columns], train[target])\n",
    "\n",
    "columns = [f\"param_{name}\" for name in param_distributions.keys()]\n",
    "columns += [\"mean_test_error\", \"std_test_error\"]\n",
    "cv_results = pd.DataFrame(search_cv.cv_results_)\n",
    "cv_results[\"mean_test_error\"] = -cv_results[\"mean_test_score\"]\n",
    "cv_results[\"std_test_error\"] = cv_results[\"std_test_score\"]\n",
    "cv_results[columns].sort_values(by=\"mean_test_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "clf = GradientBoostingRegressor(learning_rate=0.1, n_estimators = 900, max_leaf_nodes= 50, min_samples_split=25, random_state=0, subsample=0.8, max_features=17, max_depth=50)\n",
    "\n",
    "target = 'price'\n",
    "columns = train.columns.tolist()\n",
    "columns = [c for c in columns if c not in [\"price\"]]\n",
    "\n",
    "#training = train.sample(frac=0.7, random_state=5)\n",
    "#test = train.loc[~train.index.isin(training.index)]\n",
    "clf.fit(train[columns], train[target])\n",
    "clf = clf.predict(test[columns])\n",
    "# Compute error between our test predictions and the actual values.\n",
    "#lin_mse = mean_squared_error(clf, test[target], squared=False)\n",
    "#print(\"Computed error:\", lin_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for index in range(100, 120):\n",
    "    #print(lin_predictions[index], test[target].array[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"result.csv\", lin_predictions, header=['price'])\n",
    "result = pd.DataFrame(clf)\n",
    "result.to_csv(\"result.csv\", header=['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31e71d7ed0fa0b4c5d8ce44b530b41ec61e7cc3af5332ca9786b8048d514a256"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
