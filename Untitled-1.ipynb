{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import mpl_toolkits\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(file_name, price):\n",
    "    train = pd.read_csv(file_name)\n",
    "    fields = ['publication_date', 'latitude', 'longitude', 'category', 'floor', 'area', 'kupcha', 'rooms', 'repairs', 'hypothec', 'location attributes']\n",
    "    if price:\n",
    "        fields.append('price')\n",
    "    train = train[fields]\n",
    "    train.replace('var', True, inplace=True)\n",
    "    train.replace('yoxdur', False, inplace=True)\n",
    "    train.replace(np.nan, False, inplace=True)\n",
    "    train.replace({'m²': ''}, regex=True, inplace=True)\n",
    "    train.replace('Yeni tikili', True, inplace=True)\n",
    "    train.replace('Köhnə tikili', False, inplace=True)\n",
    "    train = train.astype({\"area\": float})\n",
    "    currentFloors = []\n",
    "    maxFloors = []\n",
    "    for floor in train['floor'].array:\n",
    "        floors = floor.split(' / ')\n",
    "        currentFloors.append(floors[0])\n",
    "        maxFloors.append(floors[1])\n",
    "    train[\"current_floor\"] = currentFloors\n",
    "    train[\"max_floor\"] = maxFloors\n",
    "\n",
    "    train = train.astype({\"current_floor\": int})\n",
    "    train = train.astype({\"max_floor\": int})\n",
    "    del train['floor']\n",
    "\n",
    "    def get_normalized_series(df, col):\n",
    "        return (df[col] - df[col].mean()) / df[col].std()\n",
    "    #train[\"NormArea\"]= get_normalized_series(train, \"area\")\n",
    "    #train[\"NormCurrent\"]= get_normalized_series(train, \"current_floor\")\n",
    "    #train[\"NormMax\"]= get_normalized_series(train, \"max_floor\")\n",
    "    #if price:\n",
    "       #train[\"NormPrice\"]= get_normalized_series(train, \"price\")\n",
    "       #del train['price']\n",
    "    #del train['area']\n",
    "    #del train['current_floor']\n",
    "    #del train['max_floor']\n",
    "    for index in range(0, len(train['publication_date'].array)):\n",
    "        element = train['publication_date'].array[index]\n",
    "        values = element.split(' ')\n",
    "        if values[1] == 'Yanvar':\n",
    "            values[1] = '/01/'\n",
    "        elif values[1] == 'Dekabr':\n",
    "            values[1] = '/12/'\n",
    "        train['publication_date'].array[index] = values[0] + values[1] + values[2]\n",
    "\n",
    "    train['publication_date'] = pd.to_datetime(train.publication_date, format=\"%d/%m/%Y\", errors='coerce')\n",
    "    train[\"year\"] = train[\"publication_date\"].dt.year\n",
    "    train[\"month\"] = train[\"publication_date\"].dt.month\n",
    "    train[\"day\"] = train[\"publication_date\"].dt.day\n",
    "    return train\n",
    "train = process('modified_train_binaaz.csv', True)\n",
    "tags = train[\"location attributes\"]\n",
    "clean_tags = tags.str.strip('[]\\'').str.split('\\', \\'')\n",
    "#tag_columns = pd.get_dummies(clean_tags.apply(pd.Series).stack()).groupby(level=0).sum()\n",
    "a = clean_tags.apply(pd.Series)\n",
    "b = a.stack()\n",
    "c = pd.get_dummies(b)\n",
    "d = c.groupby(level=0).sum()\n",
    "\n",
    "train = train.join(d)\n",
    "train = train.drop(['location attributes', 'publication_date'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"clear_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = process('test_binaaz_updated.csv', False)\n",
    "tags = test[\"location attributes\"]\n",
    "test_clean_tags = tags.str.strip('[]\\'').str.split('\\', \\'')\n",
    "#tag_columns = pd.get_dummies(test_clean_tags.apply(pd.Series).stack()).groupby(level=0).sum()\n",
    "a = test_clean_tags.apply(pd.Series)\n",
    "b = a.stack()\n",
    "c = pd.get_dummies(b)\n",
    "d = c.groupby(level=0).sum()\n",
    "\n",
    "tags = set()\n",
    "for ls in clean_tags:\n",
    "   for tag in ls:\n",
    "    tags.add(tag)\n",
    "\n",
    "test_tags = set()\n",
    "for ls in test_clean_tags:\n",
    "   for tag in ls:\n",
    "    test_tags.add(tag)\n",
    "\n",
    "to_add = tags.difference(test_tags)\n",
    "empty = []\n",
    "for i in range(0, len(d.index)):\n",
    "    empty.append(0)\n",
    "\n",
    "map = {'price': empty}\n",
    "for add in to_add:\n",
    "    map[add] = empty\n",
    "\n",
    "additional = pd.DataFrame(map)\n",
    "\n",
    "test = test.join(d)\n",
    "test = test.join(additional)\n",
    "to_drop = ['location attributes', 'publication_date'] + list(test_tags.difference(tags))\n",
    "test = test.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"test_clear.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "target = \"price\"\n",
    "columns = train.columns.tolist()\n",
    "columns = [c for c in columns if c not in [\"price\"]]\n",
    "#training = train.sample(frac=0.7, random_state=5)\n",
    "#lin_model = LinearRegression()\n",
    "# Fit the model to the training data.\n",
    "#lin_model.fit(train[columns], train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate our predictions for the test set.\n",
    "#test = train.loc[~train.index.isin(training.index)]\n",
    "#lin_predictions = lin_model.predict(test[columns])\n",
    "# Compute error between our test predictions and the actual values.\n",
    "#lin_mse = mean_squared_error(lin_predictions, test[target], squared=False)\n",
    "#print(\"Computed error:\", lin_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dzinski/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:292: UserWarning: The total space of parameters 6 is smaller than n_iter=12. Running 6 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.838726</td>\n",
       "      <td>0.041361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.85</td>\n",
       "      <td>-0.836915</td>\n",
       "      <td>0.046917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.836839</td>\n",
       "      <td>0.046674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.834879</td>\n",
       "      <td>0.046136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.9</td>\n",
       "      <td>-0.833659</td>\n",
       "      <td>0.047175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.824562</td>\n",
       "      <td>0.046748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_subsample  mean_test_error  std_test_error\n",
       "3             0.8        -0.838726        0.041361\n",
       "4            0.85        -0.836915        0.046917\n",
       "2            0.75        -0.836839        0.046674\n",
       "1             0.7        -0.834879        0.046136\n",
       "5             0.9        -0.833659        0.047175\n",
       "0             0.6        -0.824562        0.046748"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import loguniform\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = {\n",
    "    #\"n_estimators\": [5, 10, 20, 50, 100, 200, 300, 400, 500, 600,650, 700, 800, 900, 1000],\n",
    "    #\"max_leaf_nodes\": [2, 3, 4, 5, 10, 20, 50, 100, 150, 200, 250],\n",
    "    #\"learning_rate\": loguniform(0.01, 1),\n",
    "    #\"min_samples_split\": [10, 25, 50, 75, 100, 125, 150, 175, 200, 225, 250]\n",
    "    #\"max_depth\":range(9,20,2),\n",
    "    #\"min_samples_split\": list(range(5, 25, 5)) + list(range(25, 100, 10)) + [125, 150, 200, 250, 300]\n",
    "    #'max_features': range(47, 52, 3)\n",
    "    #\"subsample\": [0.6,0.7,0.75,0.8,0.85,0.9]\n",
    "}\n",
    "search_cv = RandomizedSearchCV(\n",
    "    GradientBoostingRegressor(max_features=50, n_estimators = 900, min_samples_split=25, max_depth=15, max_leaf_nodes= 50), param_distributions=param_distributions,\n",
    "     n_iter=12, random_state=0, n_jobs=2,\n",
    ")\n",
    "search_cv.fit(train[columns], train[target])\n",
    "\n",
    "columns = [f\"param_{name}\" for name in param_distributions.keys()]\n",
    "columns += [\"mean_test_error\", \"std_test_error\"]\n",
    "cv_results = pd.DataFrame(search_cv.cv_results_)\n",
    "cv_results[\"mean_test_error\"] = -cv_results[\"mean_test_score\"]\n",
    "cv_results[\"std_test_error\"] = cv_results[\"std_test_score\"]\n",
    "cv_results[columns].sort_values(by=\"mean_test_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "clf = GradientBoostingRegressor(learning_rate=0.1, n_estimators = 900, max_leaf_nodes= 50, min_samples_split=25, random_state=0, subsample=0.8, max_features=17, max_depth=50)\n",
    "\n",
    "target = 'price'\n",
    "columns = train.columns.tolist()\n",
    "columns = [c for c in columns if c not in [\"price\"]]\n",
    "\n",
    "#training = train.sample(frac=0.7, random_state=5)\n",
    "#test = train.loc[~train.index.isin(training.index)]\n",
    "clf.fit(train[columns], train[target])\n",
    "clf = clf.predict(test[columns])\n",
    "# Compute error between our test predictions and the actual values.\n",
    "#lin_mse = mean_squared_error(clf, test[target], squared=False)\n",
    "#print(\"Computed error:\", lin_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for index in range(100, 120):\n",
    "    #print(lin_predictions[index], test[target].array[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"result.csv\", lin_predictions, header=['price'])\n",
    "result = pd.DataFrame(clf)\n",
    "result.to_csv(\"result.csv\", header=['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2930d6c8336d5f928573f4f18e77cd4b174681c554bf1d0455ffc76e8e56d82a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
